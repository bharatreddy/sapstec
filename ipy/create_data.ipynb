{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/matplotlib/font_manager.py:273: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n",
      "  warnings.warn('Matplotlib is building the font cache using fc-list. This may take a moment.')\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "import datetime\n",
    "import time\n",
    "import urllib\n",
    "import bs4\n",
    "import os\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib import ticker\n",
    "import seaborn as sns\n",
    "import feather\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tecMinCutoff = 10.\n",
    "delTecCutoff = 0.\n",
    "delMlatCutoff = 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dst_date</th>\n",
       "      <th>dst_index</th>\n",
       "      <th>dateStr</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>20110101</td>\n",
       "      <td>01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>20110101</td>\n",
       "      <td>02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>20110101</td>\n",
       "      <td>03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>20110101</td>\n",
       "      <td>04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01 05:00:00</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>20110101</td>\n",
       "      <td>05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             dst_date  dst_index   dateStr hour\n",
       "0 2011-01-01 01:00:00      -11.0  20110101   01\n",
       "1 2011-01-01 02:00:00      -11.0  20110101   02\n",
       "2 2011-01-01 03:00:00       -9.0  20110101   03\n",
       "3 2011-01-01 04:00:00       -5.0  20110101   04\n",
       "4 2011-01-01 05:00:00       -3.0  20110101   05"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get dst index vals from wdc kyoto website\n",
    "# create a list of dates with monthly freq\n",
    "date_dst_arr = []\n",
    "dst_val = []\n",
    "dst_time_del = datetime.timedelta(hours = 1)\n",
    "start_date = datetime.datetime(2011,1,1)\n",
    "end_date = datetime.datetime(2014,12,31)\n",
    "daterange = pandas.date_range(start_date, end_date, freq=\"M\")\n",
    "for dt in daterange:\n",
    "    if dt.month <= 9:\n",
    "            monthStr = \"0\" + str(dt.month)\n",
    "    else:\n",
    "        monthStr = str(dt.month)\n",
    "    if dt.year >= 2016:\n",
    "        # create the url\n",
    "        currUrl = \"http://wdc.kugi.kyoto-u.ac.jp/\" + \"dst_realtime\" + \\\n",
    "            \"/\" + str(dt.year) + monthStr + \"/index.html\"\n",
    "    elif ( (dt.year >= 2014) and (dt.year <= 2015) ):\n",
    "        # create the url\n",
    "        currUrl = \"http://wdc.kugi.kyoto-u.ac.jp/\" + \"dst_provisional\" + \\\n",
    "            \"/\" + str(dt.year) + monthStr + \"/index.html\"\n",
    "    else:\n",
    "        # create the url\n",
    "        currUrl = \"http://wdc.kugi.kyoto-u.ac.jp/\" + \"dst_final\" + \\\n",
    "            \"/\" + str(dt.year) + monthStr + \"/index.html\"\n",
    "    conn = urllib.urlopen(currUrl)\n",
    "    htmlSource = conn.read()\n",
    "    soup = bs4.BeautifulSoup(htmlSource, 'html.parser')\n",
    "    dataResObj = soup.find(\"pre\", { \"class\" : \"data\" })\n",
    "    # get the data as a list of strings after removing white space\n",
    "    lines = dataResObj.text.strip().splitlines()\n",
    "    for line in lines[6:]:\n",
    "        columns = line.split()\n",
    "        if len( columns ) > 0. :\n",
    "            date_dst_arr.append( datetime.datetime( \\\n",
    "                dt.year, dt.month, int(columns[0]), 1 ) )\n",
    "            for cols in range( len( columns[1:] ) ) :\n",
    "                try:\n",
    "                    inNumberFloatTest = float(columns[cols + 1])\n",
    "                except:\n",
    "                    # split these cols as well and work on them!\n",
    "                    try:\n",
    "                        missedCols = columns[cols + 1].split(\"-\")[1:]\n",
    "                        if len(missedCols) >= 1:\n",
    "                            for mcols in missedCols:\n",
    "                                dst_val.append( -1*float( mcols ) )\n",
    "                                # now since we added the date earlier we need to be\n",
    "                                # careful about appending date values\n",
    "                                if ( len(date_dst_arr) != len(dst_val) ):\n",
    "                                    date_dst_arr.append ( date_dst_arr[-1] + dst_time_del )\n",
    "                    except:\n",
    "                        print \"something wrong with messed up vals!-->\", columns[cols + 1]\n",
    "                        continue\n",
    "                    continue\n",
    "                # I have to do this because of the messed up way Kyoto puts up the latest dst value..\n",
    "                # mixed with 9999 (fillers) like if latest dst is 1 then Kyoto puts it as 199999.....\n",
    "                if len( columns[ cols + 1 ] ) < 5 :\n",
    "                    dst_val.append( float( columns[ cols + 1 ] ) )\n",
    "                elif ( len( columns[ cols + 1 ] ) > 5 and columns[ cols + 1 ][0:3] != '999' ) :\n",
    "                    mixed_messed_dst = ''\n",
    "                    for jj in range(5) :\n",
    "                        if columns[ cols + 1 ][jj] != '9' :\n",
    "                            mixed_messed_dst = mixed_messed_dst + columns[ cols + 1 ][jj]\n",
    "\n",
    "                    if mixed_messed_dst != '-' :\n",
    "                        dst_val.append( float( mixed_messed_dst ) )\n",
    "                    else :\n",
    "                        dst_val.append( float( 'nan' ) )\n",
    "                else :\n",
    "                    dst_val.append( float( 'nan' ) )\n",
    "                if cols > 0 :\n",
    "                    date_dst_arr.append ( date_dst_arr[-1] + dst_time_del )\n",
    "# convert dst data to a dataframe\n",
    "dstDF = pandas.DataFrame(\n",
    "    {'dst_date': date_dst_arr,\n",
    "     'dst_index': dst_val\n",
    "    })\n",
    "dstDF[\"dateStr\"] = dstDF[\"dst_date\"].map(lambda x: x.strftime('%Y%m%d'))\n",
    "dstDF[\"hour\"] = dstDF[\"dst_date\"].map(lambda x: x.strftime('%H'))\n",
    "dstDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3b4114c92b08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mframes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mbndDF\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mfinBndDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mframes\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mfinBndDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"normMLT\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m24\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m12\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m                         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfinBndDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mlt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mfinBndDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"delTecEqu\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinBndDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tecEqu\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mfinBndDF\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tecMin\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/reshape/concat.pyc\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    204\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m                        copy=copy)\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/pandas/core/reshape/concat.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No objects to concatenate'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "baseDir = \"/home/bharat/Documents/code/data/trghBnds/\"\n",
    "\n",
    "colNames = [ \"mlatEqu\", \"tecEqu\", \"mlon\",\\\n",
    "            \"mlatPol\", \"tecPol\", \"date\",\\\n",
    "            \"mlatMin\", \"tecMin\", \"mlt\", \"mlonAdjst\" ]\n",
    "frames = []\n",
    "# cnt = 0\n",
    "for root, dirs, files in os.walk(baseDir):\n",
    "    for fNum, fName in enumerate(files):\n",
    "        currInpLosFile = root + fName\n",
    "        bndDF = pandas.read_csv(currInpLosFile, delim_whitespace=True,\\\n",
    "                                    header=None, names=colNames,\\\n",
    "                                infer_datetime_format=True,\\\n",
    "                                parse_dates=[\"date\"])\n",
    "        frames.append( bndDF )\n",
    "\n",
    "finBndDF = pandas.concat( frames )\n",
    "finBndDF[\"normMLT\"] = [x-24 if x >= 12 else x\\\n",
    "                         for x in finBndDF['mlt']]\n",
    "finBndDF[\"delTecEqu\"] = finBndDF[\"tecEqu\"] - finBndDF[\"tecMin\"]\n",
    "finBndDF[\"delTecPol\"] = finBndDF[\"tecPol\"] - finBndDF[\"tecMin\"]\n",
    "finBndDF[\"delMlat\"] = finBndDF[\"mlatPol\"] - finBndDF[\"mlatEqu\"]\n",
    "finBndDF[\"timeStr\"] = finBndDF[\"date\"].dt.strftime('%H%M').astype(int)\n",
    "# # discard dates where delTecEqu and delTecPol are -ve\n",
    "# finBndDF[\"dateStr\"] = finBndDF[\"date\"].dt.strftime('%Y%m%d')\n",
    "discrdDatesDelTec = finBndDF[ (finBndDF[\"delTecEqu\"] < delTecCutoff) |\\\n",
    "                   (finBndDF[\"delTecPol\"] < delTecCutoff) ][\"date\"].values\n",
    "finBndDF = finBndDF[ ~finBndDF[\"date\"].isin(discrdDatesDelTec) ].reset_index(drop=True)\n",
    "# Discard those dates where tecMin is greater than 10.\n",
    "discrdDatestecMin = finBndDF[ (finBndDF[\"tecMin\"] > tecMinCutoff) ][\"date\"].values\n",
    "finBndDF = finBndDF[ ~finBndDF[\"date\"].isin(discrdDatestecMin) ].reset_index(drop=True)\n",
    "# Discard locations where delMlat < 0.\n",
    "finBndDF = finBndDF[ finBndDF[\"delMlat\"] > 0. ].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(12, 8))\n",
    "ax = f.add_subplot(1,1,1)\n",
    "finBndDF.hist([\"normMLT\", \"tecMin\", \"delTecPol\", \"delTecEqu\"], bins=23, ax=ax)\n",
    "f.savefig( \"../figs/hist-plots.pdf\",bbox_inches='tight' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# finBndDF.shape\n",
    "# asyDF = pandas.read_csv( \"../data/Asy_processed.txt\", sep=' ' )\n",
    "# asyDF[\"date\"] = pandas.to_datetime(asyDF[\"datetimeStr\"], format='%Y%m%d-%H-%M')\n",
    "finBndDF[\"dateStr\"] = finBndDF[\"date\"].map(lambda x: x.strftime('%Y%m%d'))\n",
    "finBndDF[\"hour\"] = finBndDF[\"date\"].map(lambda x: x.strftime('%H'))\n",
    "finBndDF = pandas.merge( finBndDF, dstDF, on=[\"dateStr\", \"hour\"] )\n",
    "dstBins = [ -150, -75, -50, -25, -10, 10 ]\n",
    "finBndDF = pandas.concat( [ finBndDF, \\\n",
    "                    pandas.cut( finBndDF[\"dst_index\"], \\\n",
    "                               bins=dstBins ) ], axis=1 )\n",
    "finBndDF.columns = ['mlatEqu', 'tecEqu', 'mlon', 'mlatPol', 'tecPol', 'date',\n",
    "       'mlatMin', 'tecMin', 'mlt', 'mlonAdjst', 'normMLT', 'delTecEqu',\n",
    "       'delTecPol', 'delMlat', 'timeStr', 'dateStr', 'hour', 'dst_date',\n",
    "       'dst_index', 'dst_bin']\n",
    "feather.write_dataframe(finBndDF, '../data/trghBndDst.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# file containing saps data --> date, time, sapsLat, sapsMLT, sapsVel, radId, poesLat, poesMLT\n",
    "#file_sapsdata = \"/Users/bharat/Desktop/saps-north-2011-2012.txt\"\n",
    "file_sapsdata = \"../data/rawsaps-north-2011-2014.txt\"\n",
    "# store the data to convert it to DF later\n",
    "allData = []\n",
    "# open and read through the file\n",
    "fs = open(file_sapsdata, 'r')\n",
    "# only take data from mid-latitude radars\n",
    "midlatRadIds = [209, 208, 33, 207, 206, 205, 204, 32]\n",
    "for line in fs:\n",
    "    line = line.strip()\n",
    "    columns = line.split()\n",
    "    \n",
    "    dt_ind = time.strptime( columns[0], \"%Y%m%d\" )\n",
    "    hh_ind = int(int(columns[1])/100)\n",
    "    mm_ind = int(int(columns[1]) % 100)\n",
    "    currDt = datetime.datetime( dt_ind.tm_year, dt_ind.tm_mon, dt_ind.tm_mday, hh_ind, mm_ind )\n",
    "    allData.append( [ columns[0] + \"-\" + columns[1], currDt, columns[0], \\\n",
    "                     float( columns[2] ), float( columns[3] ), float( columns[4] ), \\\n",
    "                     float( columns[5] ), float( columns[6] ), float( columns[7] ) ] )  \n",
    "fs.close()\n",
    "# store data in a DF\n",
    "sapsRawDF = pandas.DataFrame(allData)\n",
    "sapsRawDF.columns = [ \"dateTimeString\", \"date\", \"dateStr\", \"sapsLat\", \\\n",
    "                     \"sapsMLT\", \"sapsVel\", \"radId\", \"poesLat\", \"poesMLT\" ]\n",
    "# count number of unique dates present in the raw DF\n",
    "uniqRawDates = sapsRawDF[\"dateStr\"].unique().tolist()\n",
    "print \"num of unique(total) dates--->\", len(uniqRawDates)\n",
    "sapsRawDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a date string and time column for the dst DF\n",
    "dstDF[\"dateStr\"] = dstDF[\"dst_date\"].map(lambda x: x.strftime('%Y%m%d'))\n",
    "dstDF[\"hour\"] = dstDF[\"dst_date\"].map(lambda x: x.strftime('%H'))\n",
    "# Make an hour column for the sapsRawDF too\n",
    "sapsRawDF[\"hour\"] = sapsRawDF[\"date\"].map(lambda x: x.strftime('%H'))\n",
    "# Now merge the dst and sapsRaw DFs\n",
    "sapsRawDF = pandas.merge( sapsRawDF, dstDF, on=[\"dateStr\", \"hour\"], how='inner' )\n",
    "sapsRawDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Try a new method to filter for the SAPS events\n",
    "# # Involves 3 filters : \n",
    "# # 1) get data from only mid-lat radars\n",
    "sapsRawDF = sapsRawDF[ sapsRawDF[\"radId\"].isin(midlatRadIds) ]\n",
    "sapsRawDF = sapsRawDF[ sapsRawDF[\"dst_index\"] <= 10.] \n",
    "# # 2) The saps event should be observed by atleast 4 mid-latitude radars on a given day\n",
    "sapsNumRadsSer = sapsRawDF.groupby( [\"dateStr\"] ).agg( {\"radId\": pandas.Series.nunique} )\n",
    "sapsNumRadsSer = sapsNumRadsSer[ sapsNumRadsSer >= 4  ].dropna().reset_index()\n",
    "sapsNumRadsSer.columns = [ \"dateStr\", \"nRads\" ]\n",
    "# Number of data points on a given date should be greater than 200!\n",
    "sapsDateTimeCount = sapsRawDF.groupby([\"dateStr\"]).count()\n",
    "sapsDateTimeCount = sapsDateTimeCount[ \\\n",
    "                        sapsDateTimeCount[\"sapsLat\"] >= 200 ][ [\"dateTimeString\"] ].reset_index()\n",
    "sapsDateTimeCount.columns = [ \"dateStr\", \"nSapsVecs\" ]\n",
    "# Merge both the data points\n",
    "sapsDateSelDF = pandas.merge( sapsDateTimeCount, sapsNumRadsSer, on=\"dateStr\" )\n",
    "# Now merge dates selected and saps raw DFs\n",
    "sapsRawDF = pandas.merge( sapsRawDF, sapsDateSelDF, on=\"dateStr\" )\n",
    "\n",
    "prcsdSapsDF = sapsRawDF[ [\"dateStr\", \"hour\",\"sapsLat\", \\\n",
    "                     \"sapsMLT\", \"sapsVel\", \"radId\", \"poesLat\", \"poesMLT\", \"dst_date\", \"dst_index\"] ]\n",
    "\n",
    "prcsdSapsDF[\"time\"] = sapsRawDF[\"date\"].map(lambda x: x.strftime('%H%M'))\n",
    "# Save to a new model file\n",
    "feather.write_dataframe(prcsdSapsDF, '../data/processedSaps.feather')\n",
    "prcsdSapsDF.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
